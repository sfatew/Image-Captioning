{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1462296,"sourceType":"datasetVersion","datasetId":857191}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport h5py\nimport torch\nfrom tqdm import tqdm\nfrom collections import Counter\nfrom random import seed, choice, sample\nimport json\nfrom collections import defaultdict\nimport shutil\nimport random","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-30T17:27:09.810437Z","iopub.execute_input":"2024-12-30T17:27:09.810854Z","iopub.status.idle":"2024-12-30T17:27:11.905275Z","shell.execute_reply.started":"2024-12-30T17:27:09.810823Z","shell.execute_reply":"2024-12-30T17:27:11.904321Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"## UTILS","metadata":{}},{"cell_type":"code","source":"with open('/kaggle/input/coco-2017-dataset/coco2017/annotations/captions_train2017.json', 'r') as j:\n    train = json.load(j)\nwith open('/kaggle/input/coco-2017-dataset/coco2017/annotations/captions_val2017.json', 'r') as j1:\n    val = json.load(j1)\n\ndef process_and_save(file, output_path, label):\n    output_data = {\n        'images': []\n    }\n    \n    annotations_by_image = defaultdict(list)\n    for annotation in file['annotations']:\n        annotations_by_image[annotation['image_id']].append(annotation)\n\n    for image in file['images']:\n        imgid = image['id']\n        filename = image['file_name']\n\n        captions_for_image = []\n        for annotation in annotations_by_image[imgid]:\n            caption_info = {\n                'tokens': annotation['caption'].split(),\n                'raw': annotation['caption'],\n                'imgid': imgid,\n                'sentid': annotation['id']\n            }\n            captions_for_image.append(caption_info)\n        \n        image_data = {\n            'filepath': label + '2017',\n            'sentids': [caption['sentid'] for caption in captions_for_image],\n            'filename': filename,\n            'imgid': imgid,\n            'split': label,\n            'sentences': captions_for_image\n        }\n        output_data['images'].append(image_data)\n    \n    with open(output_path, 'w') as output_file:\n        json.dump(output_data, output_file, indent=4)\n\nprocess_and_save(train, '/kaggle/working/train_input.json', 'train')\nprocess_and_save(val, '/kaggle/working/val_input.json', 'val')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T13:23:44.752342Z","iopub.execute_input":"2024-12-30T13:23:44.752922Z","iopub.status.idle":"2024-12-30T13:24:08.706136Z","shell.execute_reply.started":"2024-12-30T13:23:44.752883Z","shell.execute_reply":"2024-12-30T13:24:08.704149Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"def create_input_files(captions_per_image=5, min_word_freq=5, max_len = 50):\n    \"\"\"\n    Creates input files for training, validation, and test data.\n    :param captions_per_image: number of captions to sample per image\n    :param min_word_freq: words occuring less frequently than this threshold are binned as <unk>s\n    :param max_len: don't sample captions longer than this length\n    \"\"\"\n    train_image_paths = []\n    train_image_captions = []\n    val_image_paths = []\n    val_image_captions = []\n    word_freq = Counter()\n\n    def func1(path):\n        with open(path, 'r') as j:\n            data = json.load(j)\n    \n        for img in data['images']:\n            captions = []\n            for c in img['sentences']:\n                # Update word frequency\n                word_freq.update(c['tokens'])\n                if len(c['tokens']) <= max_len:\n                    captions.append(c['tokens'])\n    \n            if len(captions) == 0:\n                continue\n                \n            path1 = os.path.join('/kaggle/input/coco-2017-dataset/coco2017/', img['filepath'], img['filename'])\n    \n            if img['split'] in {'train'}:\n                train_image_paths.append(path1)\n                train_image_captions.append(captions)\n            elif img['split'] in {'val'}:\n                val_image_paths.append(path1)\n                val_image_captions.append(captions)\n\n    func1('/kaggle/working/train_input.json')\n    func1('/kaggle/working/val_input.json')\n    \n    # Create word map\n    words = [w for w in word_freq.keys() if word_freq[w] > min_word_freq]\n    word_map = {key: value + 1 for value, key in enumerate(words)}\n    \n    word_map['<start>'] = len(word_map) + 1\n    word_map['<unk>'] = len(word_map) + 1\n    word_map['<end>'] = len(word_map) + 1\n    word_map['<pad>'] = 0\n\n    # Create a base/root name for all output files\n    base_filename = 'coco' + '_' + str(captions_per_image) + '_cap_per_img_' + str(min_word_freq) + '_min_word_freq'\n\n    # Save word map to a JSON\n    with open(os.path.join('/kaggle/working/', 'WORDMAP_' + base_filename + '.json'), 'w') as j:\n        json.dump(word_map, j)\n\n    seed(123)\n    for impaths, imcaps, split in [(val_image_paths, val_image_captions, 'VAL'), (train_image_paths, train_image_captions, 'TRAIN')]:\n\n        with h5py.File(os.path.join('/kaggle/working/', split + '_IMAGES_' + base_filename + '.hdf5'), 'a') as h:\n            # Make a note of the number of captions we are sampling per image\n            h.attrs['captions_per_image'] = captions_per_image\n\n            images = h.create_dataset('images', (len(impaths), 3, 256, 256), dtype='uint8')\n\n            print(\"\\nReading %s images and captions, storing to file...\\n\" % split)\n\n            enc_captions = []\n            caplens = []\n\n            for i, path in enumerate(tqdm(impaths)):\n\n                # Sample captions\n                if len(imcaps[i]) < captions_per_image:\n                    captions = imcaps[i] + [choice(imcaps[i]) for _ in range(captions_per_image - len(imcaps[i]))]\n                else:\n                    captions = sample(imcaps[i], k=captions_per_image)\n\n                # Sanity check\n                assert len(captions) == captions_per_image\n\n                # Read images\n                img = Image.open(impaths[i])\n                if img.mode != 'RGB': \n                    img = img.convert('RGB') \n                # Resize image to 256x256\n                img = img.resize((256, 256))\n                # Convert the image to a numpy array and change the dimension order (H, W, C -> C, H, W)\n                img = np.array(img).transpose(2, 0, 1)\n\n                # Save image to HDF5 file\n                images[i] = img\n\n                for j, c in enumerate(captions):\n                    # Encode captions\n                    enc_c = [word_map['<start>']] + [word_map.get(word, word_map['<unk>']) for word in c] + [\n                        word_map['<end>']] + [word_map['<pad>']] * (max_len - len(c))\n\n                    # Find caption lengths\n                    c_len = len(c) + 2\n\n                    enc_captions.append(enc_c)\n                    caplens.append(c_len)\n\n            # Save encoded captions and their lengths to JSON files\n            with open(os.path.join('/kaggle/working/', split + '_CAPTIONS_' + base_filename + '.json'), 'w') as j:\n                json.dump(enc_captions, j)\n\n            with open(os.path.join('/kaggle/working/', split + '_CAPLENS_' + base_filename + '.json'), 'w') as j:\n                json.dump(caplens, j)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T13:24:08.709581Z","iopub.execute_input":"2024-12-30T13:24:08.710183Z","iopub.status.idle":"2024-12-30T13:24:08.728719Z","shell.execute_reply.started":"2024-12-30T13:24:08.710148Z","shell.execute_reply":"2024-12-30T13:24:08.727486Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"create_input_files(captions_per_image=5, min_word_freq=5, max_len = 50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T13:24:08.730870Z","iopub.execute_input":"2024-12-30T13:24:08.731286Z","iopub.status.idle":"2024-12-30T14:00:13.427778Z","shell.execute_reply.started":"2024-12-30T13:24:08.731256Z","shell.execute_reply":"2024-12-30T14:00:13.424067Z"}},"outputs":[{"name":"stdout","text":"\nReading VAL images and captions, storing to file...\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5000/5000 [01:51<00:00, 44.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nReading TRAIN images and captions, storing to file...\n\n","output_type":"stream"},{"name":"stderr","text":" 84%|████████▍ | 99427/118287 [34:03<06:27, 48.66it/s]  \n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-fedcdae68f4b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcreate_input_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaptions_per_image\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_word_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-3-4b9bb4c7b96f>\u001b[0m in \u001b[0;36mcreate_input_files\u001b[0;34m(captions_per_image, min_word_freq, max_len)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0;31m# Save image to HDF5 file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                 \u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n","\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/h5py/_hl/dataset.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, args, val)\u001b[0m\n\u001b[1;32m    997\u001b[0m         \u001b[0mmspace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_simple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfspace\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mselection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 999\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdxpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dxpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread_direct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_sel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest_sel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n","\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n","\u001b[0;32mh5py/h5d.pyx\u001b[0m in \u001b[0;36mh5py.h5d.DatasetID.write\u001b[0;34m()\u001b[0m\n","\u001b[0;32mh5py/_proxy.pyx\u001b[0m in \u001b[0;36mh5py._proxy.dset_rw\u001b[0;34m()\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: [Errno 28] Can't synchronously write data (file write failed: time = Mon Dec 30 14:00:13 2024\n, filename = '/kaggle/working/TRAIN_IMAGES_coco_5_cap_per_img_5_min_word_freq.hdf5', file descriptor = 42, errno = 28, error message = 'No space left on device', buf = 0x5cc09e4da120, total write size = 67584, bytes this sub-write = 67584, bytes actually written = 18446744073709551615, offset = 0)"],"ename":"OSError","evalue":"[Errno 28] Can't synchronously write data (file write failed: time = Mon Dec 30 14:00:13 2024\n, filename = '/kaggle/working/TRAIN_IMAGES_coco_5_cap_per_img_5_min_word_freq.hdf5', file descriptor = 42, errno = 28, error message = 'No space left on device', buf = 0x5cc09e4da120, total write size = 67584, bytes this sub-write = 67584, bytes actually written = 18446744073709551615, offset = 0)","output_type":"error"}],"execution_count":4},{"cell_type":"code","source":"def init_embedding(embeddings):\n    \"\"\"\n    Fills embedding tensor with values from the uniform distribution.\n\n    :param embeddings: embedding tensor\n    \"\"\"\n    bias = np.sqrt(3.0 / embeddings.size(1))\n    torch.nn.init.uniform_(embeddings, -bias, bias)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T14:04:56.220433Z","iopub.execute_input":"2024-12-30T14:04:56.220842Z","iopub.status.idle":"2024-12-30T14:04:56.226232Z","shell.execute_reply.started":"2024-12-30T14:04:56.220814Z","shell.execute_reply":"2024-12-30T14:04:56.225173Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def load_embeddings(emb_file, word_map):\n    \"\"\"\n    Creates an embedding tensor for the specified word map, for loading into the model.\n\n    :param emb_file: file containing embeddings (stored in GloVe format)\n    :param word_map: word map\n    :return: embeddings in the same order as the words in the word map, dimension of embeddings\n    \"\"\"\n\n    # Find embedding dimension\n    with open(emb_file, 'r') as f:\n        emb_dim = len(f.readline().split(' ')) - 1\n\n    vocab = set(word_map.keys())\n\n    # Create tensor to hold embeddings, initialize\n    embeddings = torch.FloatTensor(len(vocab), emb_dim)\n    init_embedding(embeddings)\n\n    # Read embedding file\n    print(\"\\nLoading embeddings...\")\n    for line in open(emb_file, 'r'):\n        line = line.split(' ')\n\n        emb_word = line[0]\n        embedding = list(map(lambda t: float(t), filter(lambda n: n and not n.isspace(), line[1:])))\n\n        # Ignore word if not in train_vocab\n        if emb_word not in vocab:\n            continue\n\n        embeddings[word_map[emb_word]] = torch.FloatTensor(embedding)\n\n    return embeddings, emb_dim\n\n\ndef clip_gradient(optimizer, grad_clip):\n    \"\"\"\n    Clips gradients computed during backpropagation to avoid explosion of gradients.\n\n    :param optimizer: optimizer with the gradients to be clipped\n    :param grad_clip: clip value\n    \"\"\"\n    for group in optimizer.param_groups:\n        for param in group['params']:\n            if param.grad is not None:\n                param.grad.data.clamp_(-grad_clip, grad_clip)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T14:05:00.778854Z","iopub.execute_input":"2024-12-30T14:05:00.779235Z","iopub.status.idle":"2024-12-30T14:05:00.787272Z","shell.execute_reply.started":"2024-12-30T14:05:00.779206Z","shell.execute_reply":"2024-12-30T14:05:00.785986Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def save_checkpoint(epoch, epochs_since_improvement, encoder, decoder, encoder_optimizer, decoder_optimizer,\n                    bleu4, is_best):\n    \"\"\"\n    Saves model checkpoint.\n\n    :param data_name: base name of processed dataset\n    :param epoch: epoch number\n    :param epochs_since_improvement: number of epochs since last improvement in BLEU-4 score\n    :param encoder: encoder model\n    :param decoder: decoder model\n    :param encoder_optimizer: optimizer to update encoder's weights, if fine-tuning\n    :param decoder_optimizer: optimizer to update decoder's weights\n    :param bleu4: validation BLEU-4 score for this epoch\n    :param is_best: is this checkpoint the best so far?\n    \"\"\"\n    state = {'epoch': epoch,\n             'epochs_since_improvement': epochs_since_improvement,\n             'bleu-4': bleu4,\n             'encoder': encoder,\n             'decoder': decoder,\n             'encoder_optimizer': encoder_optimizer,\n             'decoder_optimizer': decoder_optimizer}\n    filename = 'checkpoint_' + '.pth.tar'\n    torch.save(state, filename)\n    # If this checkpoint is the best so far, store a copy so it doesn't get overwritten by a worse checkpoint\n    if is_best:\n        torch.save(state, 'BEST_' + filename)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T14:05:08.398507Z","iopub.execute_input":"2024-12-30T14:05:08.399127Z","iopub.status.idle":"2024-12-30T14:05:08.405356Z","shell.execute_reply.started":"2024-12-30T14:05:08.399091Z","shell.execute_reply":"2024-12-30T14:05:08.404051Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"class AverageMeter(object):\n    \"\"\"\n    Keeps track of most recent, average, sum, and count of a metric.\n    \"\"\"\n\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T14:05:12.801345Z","iopub.execute_input":"2024-12-30T14:05:12.801710Z","iopub.status.idle":"2024-12-30T14:05:12.807862Z","shell.execute_reply.started":"2024-12-30T14:05:12.801670Z","shell.execute_reply":"2024-12-30T14:05:12.806460Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def adjust_learning_rate(optimizer, shrink_factor):\n    \"\"\"\n    Shrinks learning rate by a specified factor.\n\n    :param optimizer: optimizer whose learning rate must be shrunk.\n    :param shrink_factor: factor in interval (0, 1) to multiply learning rate with.\n    \"\"\"\n\n    print(\"\\nDECAYING learning rate.\")\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = param_group['lr'] * shrink_factor\n    print(\"The new learning rate is %f\\n\" % (optimizer.param_groups[0]['lr'],))\n\n\ndef accuracy(scores, targets, k):\n    \"\"\"\n    Computes top-k accuracy, from predicted and true labels.\n\n    :param scores: scores from the model\n    :param targets: true labels\n    :param k: k in top-k accuracy\n    :return: top-k accuracy\n    \"\"\"\n\n    batch_size = targets.size(0)\n    _, ind = scores.topk(k, 1, True, True)\n    correct = ind.eq(targets.view(-1, 1).expand_as(ind))\n    correct_total = correct.view(-1).float().sum()  # 0D tensor\n    return correct_total.item() * (100.0 / batch_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T14:05:15.100242Z","iopub.execute_input":"2024-12-30T14:05:15.100605Z","iopub.status.idle":"2024-12-30T14:05:15.109319Z","shell.execute_reply.started":"2024-12-30T14:05:15.100564Z","shell.execute_reply":"2024-12-30T14:05:15.108110Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"from torch.utils.data import Dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T14:05:18.750142Z","iopub.execute_input":"2024-12-30T14:05:18.750606Z","iopub.status.idle":"2024-12-30T14:05:18.756138Z","shell.execute_reply.started":"2024-12-30T14:05:18.750563Z","shell.execute_reply":"2024-12-30T14:05:18.754916Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"## Dataset","metadata":{}},{"cell_type":"code","source":"class CaptionDataset(Dataset):\n    \"\"\"\n    A PyTorch Dataset class to be used in a PyTorch DataLoader to create batches.\n    \"\"\"\n\n    def __init__(self, data_folder, data_name, split, transform=None):\n        \"\"\"\n        :param data_folder: folder where data files are stored\n        :param data_name: base name of processed datasets\n        :param split: split, one of 'TRAIN', 'VAL'\n        :param transform: image transform pipeline\n        \"\"\"\n        self.split = split\n        # Open hdf5 file where images are stored\n        self.h = h5py.File(os.path.join(data_folder, self.split + '_IMAGES_' + data_name + '.hdf5'), 'r')\n        self.imgs = self.h['images']\n\n        # Captions per image\n        self.cpi = self.h.attrs['captions_per_image']\n\n        # Load encoded captions (completely into memory)\n        with open(os.path.join(data_folder, self.split + '_CAPTIONS_' + data_name + '.json'), 'r') as j:\n            self.captions = json.load(j)\n\n        # Load caption lengths (completely into memory)\n        with open(os.path.join(data_folder, self.split + '_CAPLENS_' + data_name + '.json'), 'r') as j:\n            self.caplens = json.load(j)\n\n        # PyTorch transformation pipeline for the image (normalizing, etc.)\n        self.transform = transform\n\n        # Total number of datapoints\n        self.dataset_size = len(self.captions)\n\n    def __getitem__(self, i):\n        # Remember, the Nth caption corresponds to the (N // captions_per_image)th image\n        img = torch.FloatTensor(self.imgs[i // self.cpi] / 255.)\n        if self.transform is not None:\n            img = self.transform(img)\n\n        caption = torch.LongTensor(self.captions[i])\n\n        caplen = torch.LongTensor([self.caplens[i]])\n\n        if self.split == 'TRAIN':\n            return img, caption, caplen\n        else:\n            # For validation of testing, also return all 'captions_per_image' captions to find BLEU-4 score\n            all_captions = torch.LongTensor(\n                self.captions[((i // self.cpi) * self.cpi):(((i // self.cpi) * self.cpi) + self.cpi)])\n            return img, caption, caplen, all_captions\n\n    def __len__(self):\n        return self.dataset_size","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T14:05:20.961859Z","iopub.execute_input":"2024-12-30T14:05:20.962328Z","iopub.status.idle":"2024-12-30T14:05:20.972064Z","shell.execute_reply.started":"2024-12-30T14:05:20.962289Z","shell.execute_reply":"2024-12-30T14:05:20.970672Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"import torch\nfrom torch import nn\nimport torchvision\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T14:05:25.681059Z","iopub.execute_input":"2024-12-30T14:05:25.681385Z","iopub.status.idle":"2024-12-30T14:05:27.562575Z","shell.execute_reply.started":"2024-12-30T14:05:25.681358Z","shell.execute_reply":"2024-12-30T14:05:27.561452Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, encoded_image_size=14):\n        super(Encoder, self).__init__()\n        self.enc_image_size = encoded_image_size\n\n        resnet = torchvision.models.resnet101(pretrained=True)  # pretrained ImageNet ResNet-101\n\n        # Remove linear and pool layers (since we're not doing classification)\n        modules = list(resnet.children())[:-2]\n        self.resnet = nn.Sequential(*modules)\n\n        # Resize image to fixed size to allow input images of variable size\n        self.adaptive_pool = nn.AdaptiveAvgPool2d((encoded_image_size, encoded_image_size))\n\n        self.fine_tune()\n\n    def forward(self, images):\n        \"\"\"\n        Forward propagation.\n\n        :param images: images, a tensor of dimensions (batch_size, 3, image_size, image_size)\n        :return: encoded images\n        \"\"\"\n        out = self.resnet(images)  # (batch_size, 2048, image_size/32, image_size/32)\n        out = self.adaptive_pool(out)  # (batch_size, 2048, encoded_image_size, encoded_image_size)\n        out = out.permute(0, 2, 3, 1)  # (batch_size, encoded_image_size, encoded_image_size, 2048)\n        return out\n\n    def fine_tune(self, fine_tune=True):\n        \"\"\"\n        Allow or prevent the computation of gradients for convolutional blocks 2 through 4 of the encoder.\n\n        :param fine_tune: Allow?\n        \"\"\"\n        for p in self.resnet.parameters():\n            p.requires_grad = False\n        # If fine-tuning, only fine-tune convolutional blocks 2 through 4\n        for c in list(self.resnet.children())[5:]:\n            for p in c.parameters():\n                p.requires_grad = fine_tune","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T14:05:30.680276Z","iopub.execute_input":"2024-12-30T14:05:30.680831Z","iopub.status.idle":"2024-12-30T14:05:30.688974Z","shell.execute_reply.started":"2024-12-30T14:05:30.680797Z","shell.execute_reply":"2024-12-30T14:05:30.687875Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"class Attention(nn.Module):\n    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n        \"\"\"\n        :param encoder_dim: feature size of encoded images\n        :param decoder_dim: size of decoder's RNN\n        :param attention_dim: size of the attention network\n        \"\"\"\n        super(Attention, self).__init__()\n        self.encoder_att = nn.Linear(encoder_dim, attention_dim)  # linear layer to transform encoded image\n        self.decoder_att = nn.Linear(decoder_dim, attention_dim)  # linear layer to transform decoder's output\n        self.full_att = nn.Linear(attention_dim, 1)  # linear layer to calculate values to be softmax-ed\n        self.relu = nn.ReLU()\n        self.softmax = nn.Softmax(dim=1)  # softmax layer to calculate weights\n\n    def forward(self, encoder_out, decoder_hidden):\n        \"\"\"\n        Forward propagation.\n\n        :param encoder_out: encoded images, a tensor of dimension (batch_size, num_pixels, encoder_dim)\n        :param decoder_hidden: previous decoder output, a tensor of dimension (batch_size, decoder_dim)\n        :return: attention weighted encoding, weights\n        \"\"\"\n        att1 = self.encoder_att(encoder_out)  # (batch_size, num_pixels, attention_dim)\n        att2 = self.decoder_att(decoder_hidden)  # (batch_size, attention_dim)\n        att = self.full_att(self.relu(att1 + att2.unsqueeze(1))).squeeze(2)  # (batch_size, num_pixels)\n        alpha = self.softmax(att)  # (batch_size, num_pixels)\n        attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)  # (batch_size, encoder_dim)\n\n        return attention_weighted_encoding, alpha","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T14:05:35.660206Z","iopub.execute_input":"2024-12-30T14:05:35.660525Z","iopub.status.idle":"2024-12-30T14:05:35.667938Z","shell.execute_reply.started":"2024-12-30T14:05:35.660500Z","shell.execute_reply":"2024-12-30T14:05:35.666605Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"class DecoderWithAttention(nn.Module):\n    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size, encoder_dim=2048, dropout=0.5):\n        \"\"\"\n        :param attention_dim: size of attention network\n        :param embed_dim: embedding size\n        :param decoder_dim: size of decoder's RNN\n        :param vocab_size: size of vocabulary\n        :param encoder_dim: feature size of encoded images\n        :param dropout: dropout\n        \"\"\"\n        super(DecoderWithAttention, self).__init__()\n\n        self.encoder_dim = encoder_dim\n        self.attention_dim = attention_dim\n        self.embed_dim = embed_dim\n        self.decoder_dim = decoder_dim\n        self.vocab_size = vocab_size\n        self.dropout = dropout\n\n        self.attention = Attention(encoder_dim, decoder_dim, attention_dim)  # attention network\n\n        self.embedding = nn.Embedding(vocab_size, embed_dim)  # embedding layer\n        self.dropout = nn.Dropout(p=self.dropout)\n        self.decode_step = nn.LSTMCell(embed_dim + encoder_dim, decoder_dim, bias=True)  # decoding LSTMCell\n        self.init_h = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial hidden state of LSTMCell\n        self.init_c = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial cell state of LSTMCell\n        self.f_beta = nn.Linear(decoder_dim, encoder_dim)  # linear layer to create a sigmoid-activated gate\n        self.sigmoid = nn.Sigmoid()\n        self.fc = nn.Linear(decoder_dim, vocab_size)  # linear layer to find scores over vocabulary\n        self.init_weights()  # initialize some layers with the uniform distribution\n\n    def init_weights(self):\n        \"\"\"\n        Initializes some parameters with values from the uniform distribution, for easier convergence.\n        \"\"\"\n        self.embedding.weight.data.uniform_(-0.1, 0.1)\n        self.fc.bias.data.fill_(0)\n        self.fc.weight.data.uniform_(-0.1, 0.1)\n\n    def load_pretrained_embeddings(self, embeddings):\n        \"\"\"\n        Loads embedding layer with pre-trained embeddings.\n\n        :param embeddings: pre-trained embeddings\n        \"\"\"\n        self.embedding.weight = nn.Parameter(embeddings)\n\n    def fine_tune_embeddings(self, fine_tune=True):\n        \"\"\"\n        Allow fine-tuning of embedding layer? (Only makes sense to not-allow if using pre-trained embeddings).\n\n        :param fine_tune: Allow?\n        \"\"\"\n        for p in self.embedding.parameters():\n            p.requires_grad = fine_tune\n\n    def init_hidden_state(self, encoder_out):\n        \"\"\"\n        Creates the initial hidden and cell states for the decoder's LSTM based on the encoded images.\n\n        :param encoder_out: encoded images, a tensor of dimension (batch_size, num_pixels, encoder_dim)\n        :return: hidden state, cell state\n        \"\"\"\n        mean_encoder_out = encoder_out.mean(dim=1)\n        h = self.init_h(mean_encoder_out)  # (batch_size, decoder_dim)\n        c = self.init_c(mean_encoder_out)\n        return h, c\n\n    def forward(self, encoder_out, encoded_captions, caption_lengths):\n        \"\"\"\n        Forward propagation.\n\n        :param encoder_out: encoded images, a tensor of dimension (batch_size, enc_image_size, enc_image_size, encoder_dim)\n        :param encoded_captions: encoded captions, a tensor of dimension (batch_size, max_caption_length)\n        :param caption_lengths: caption lengths, a tensor of dimension (batch_size, 1)\n        :return: scores for vocabulary, sorted encoded captions, decode lengths, weights, sort indices\n        \"\"\"\n\n        batch_size = encoder_out.size(0)\n        encoder_dim = encoder_out.size(-1)\n        vocab_size = self.vocab_size\n\n        # Flatten image\n        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)  # (batch_size, num_pixels, encoder_dim)\n        num_pixels = encoder_out.size(1)\n\n        # Sort input data by decreasing lengths; why? apparent below\n        caption_lengths, sort_ind = caption_lengths.squeeze(1).sort(dim=0, descending=True)\n        encoder_out = encoder_out[sort_ind]\n        encoded_captions = encoded_captions[sort_ind]\n\n        # Embedding\n        embeddings = self.embedding(encoded_captions)  # (batch_size, max_caption_length, embed_dim)\n\n        # Initialize LSTM state\n        h, c = self.init_hidden_state(encoder_out)  # (batch_size, decoder_dim)\n\n        # We won't decode at the <end> position, since we've finished generating as soon as we generate <end>\n        # So, decoding lengths are actual lengths - 1\n        decode_lengths = (caption_lengths - 1).tolist()\n\n        # Create tensors to hold word predicion scores and alphas\n        predictions = torch.zeros(batch_size, max(decode_lengths), vocab_size).to(device)\n        alphas = torch.zeros(batch_size, max(decode_lengths), num_pixels).to(device)\n\n        # At each time-step, decode by\n        # attention-weighing the encoder's output based on the decoder's previous hidden state output\n        # then generate a new word in the decoder with the previous word and the attention weighted encoding\n        for t in range(max(decode_lengths)):\n            batch_size_t = sum([l > t for l in decode_lengths])\n            attention_weighted_encoding, alpha = self.attention(encoder_out[:batch_size_t],\n                                                                h[:batch_size_t])\n            gate = self.sigmoid(self.f_beta(h[:batch_size_t]))  # gating scalar, (batch_size_t, encoder_dim)\n            attention_weighted_encoding = gate * attention_weighted_encoding\n            h, c = self.decode_step(\n                torch.cat([embeddings[:batch_size_t, t, :], attention_weighted_encoding], dim=1),\n                (h[:batch_size_t], c[:batch_size_t]))  # (batch_size_t, decoder_dim)\n            preds = self.fc(self.dropout(h))  # (batch_size_t, vocab_size)\n            predictions[:batch_size_t, t, :] = preds\n            alphas[:batch_size_t, t, :] = alpha\n\n        return predictions, encoded_captions, decode_lengths, alphas, sort_ind","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T14:05:38.471957Z","iopub.execute_input":"2024-12-30T14:05:38.472490Z","iopub.status.idle":"2024-12-30T14:05:38.487397Z","shell.execute_reply.started":"2024-12-30T14:05:38.472446Z","shell.execute_reply":"2024-12-30T14:05:38.486144Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"import time\nimport torch.backends.cudnn as cudnn\nimport torch.optim\nimport torch.utils.data\nimport torchvision.transforms as transforms\nfrom torch import nn\nfrom torch.nn.utils.rnn import pack_padded_sequence\nfrom datasets import *\nfrom nltk.translate.bleu_score import corpus_bleu\n\n# Data parameters\ndata_folder = '/kaggle/working/'  # folder with data files saved by create_input_files.py\ndata_name = 'coco_5_cap_per_img_5_min_word_freq'  # base name shared by data files\n\n# Model parameters\nemb_dim = 512  # dimension of word embeddings\nattention_dim = 512  # dimension of attention linear layers\ndecoder_dim = 512  # dimension of decoder RNN\ndropout = 0.5\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # sets device for model and PyTorch tensors\ncudnn.benchmark = True  # set to true only if inputs to model are fixed size; otherwise lot of computational overhead\n\n# Training parameters\nstart_epoch = 0\nepochs = 120  # number of epochs to train for (if early stopping is not triggered)\nepochs_since_improvement = 0  # keeps track of number of epochs since there's been an improvement in validation BLEU\nbatch_size = 32\nworkers = 1  # for data-loading; right now, only 1 works with h5py\nencoder_lr = 1e-4  # learning rate for encoder if fine-tuning\ndecoder_lr = 4e-4  # learning rate for decoder\ngrad_clip = 5.  # clip gradients at an absolute value of\nalpha_c = 1.  # regularization parameter for 'doubly stochastic attention', as in the paper\nbest_bleu4 = 0.  # BLEU-4 score right now\nprint_freq = 100  # print training/validation stats every __ batches\nfine_tune_encoder = False  # fine-tune encoder?\ncheckpoint = None  # path to checkpoint, None if none","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T14:05:46.253428Z","iopub.execute_input":"2024-12-30T14:05:46.253807Z","iopub.status.idle":"2024-12-30T14:05:50.282908Z","shell.execute_reply.started":"2024-12-30T14:05:46.253777Z","shell.execute_reply":"2024-12-30T14:05:50.281831Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"def main():\n    \"\"\"\n    Training and validation.\n    \"\"\"\n    global best_bleu4, epochs_since_improvement, checkpoint, start_epoch, fine_tune_encoder, data_name, word_map\n\n    # Read word map\n    word_map_file = os.path.join(data_folder, 'WORDMAP_' + data_name + '.json')\n    with open(word_map_file, 'r') as j:\n        word_map = json.load(j)\n\n    # Initialize / load checkpoint\n    if checkpoint is None:\n        decoder = DecoderWithAttention(attention_dim=attention_dim,\n                                       embed_dim=emb_dim,\n                                       decoder_dim=decoder_dim,\n                                       vocab_size=len(word_map),\n                                       dropout=dropout)\n        decoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, decoder.parameters()),\n                                             lr=decoder_lr)\n        encoder = Encoder()\n        encoder.fine_tune(fine_tune_encoder)\n        encoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, encoder.parameters()),\n                                             lr=encoder_lr) if fine_tune_encoder else None\n\n    else:\n        checkpoint = torch.load(checkpoint)\n        start_epoch = checkpoint['epoch'] + 1\n        epochs_since_improvement = checkpoint['epochs_since_improvement']\n        best_bleu4 = checkpoint['bleu-4']\n        decoder = checkpoint['decoder']\n        decoder_optimizer = checkpoint['decoder_optimizer']\n        encoder = checkpoint['encoder']\n        encoder_optimizer = checkpoint['encoder_optimizer']\n        if fine_tune_encoder is True and encoder_optimizer is None:\n            encoder.fine_tune(fine_tune_encoder)\n            encoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, encoder.parameters()),\n                                                 lr=encoder_lr)\n\n    # Move to GPU, if available\n    decoder = decoder.to(device)\n    encoder = encoder.to(device)\n\n    # Loss function\n    criterion = nn.CrossEntropyLoss().to(device)\n\n    # Custom dataloaders\n    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                     std=[0.229, 0.224, 0.225])\n    train_loader = torch.utils.data.DataLoader(\n        CaptionDataset(data_folder, data_name, 'TRAIN', transform=transforms.Compose([normalize])),\n        batch_size=batch_size, shuffle=True, num_workers=workers, pin_memory=True)\n    val_loader = torch.utils.data.DataLoader(\n        CaptionDataset(data_folder, data_name, 'VAL', transform=transforms.Compose([normalize])),\n        batch_size=batch_size, shuffle=True, num_workers=workers, pin_memory=True)\n\n    # Epochs\n    for epoch in range(start_epoch, epochs):\n\n        # Decay learning rate if there is no improvement for 8 consecutive epochs, and terminate training after 20\n        if epochs_since_improvement == 20:\n            break\n        if epochs_since_improvement > 0 and epochs_since_improvement % 8 == 0:\n            adjust_learning_rate(decoder_optimizer, 0.8)\n            if fine_tune_encoder:\n                adjust_learning_rate(encoder_optimizer, 0.8)\n\n        # One epoch's training\n        train(train_loader=train_loader,\n              encoder=encoder,\n              decoder=decoder,\n              criterion=criterion,\n              encoder_optimizer=encoder_optimizer,\n              decoder_optimizer=decoder_optimizer,\n              epoch=epoch)\n\n        # One epoch's validation\n        recent_bleu4 = validate(val_loader=val_loader,\n                                encoder=encoder,\n                                decoder=decoder,\n                                criterion=criterion)\n\n        # Check if there was an improvement\n        is_best = recent_bleu4 > best_bleu4\n        best_bleu4 = max(recent_bleu4, best_bleu4)\n        if not is_best:\n            epochs_since_improvement += 1\n            print(\"\\nEpochs since last improvement: %d\\n\" % (epochs_since_improvement,))\n        else:\n            epochs_since_improvement = 0\n\n        # Save checkpoint\n        save_checkpoint(data_name, epoch, epochs_since_improvement, encoder, decoder, encoder_optimizer,\n                        decoder_optimizer, recent_bleu4, is_best)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T14:05:59.033843Z","iopub.execute_input":"2024-12-30T14:05:59.034729Z","iopub.status.idle":"2024-12-30T14:05:59.050478Z","shell.execute_reply.started":"2024-12-30T14:05:59.034642Z","shell.execute_reply":"2024-12-30T14:05:59.048987Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"def train(train_loader, encoder, decoder, criterion, encoder_optimizer, decoder_optimizer, epoch):\n    \"\"\"\n    Performs one epoch's training.\n\n    :param train_loader: DataLoader for training data\n    :param encoder: encoder model\n    :param decoder: decoder model\n    :param criterion: loss layer\n    :param encoder_optimizer: optimizer to update encoder's weights (if fine-tuning)\n    :param decoder_optimizer: optimizer to update decoder's weights\n    :param epoch: epoch number\n    \"\"\"\n\n    decoder.train()  # train mode (dropout and batchnorm is used)\n    encoder.train()\n\n    batch_time = AverageMeter()  # forward prop. + back prop. time\n    data_time = AverageMeter()  # data loading time\n    losses = AverageMeter()  # loss (per word decoded)\n    top5accs = AverageMeter()  # top5 accuracy\n\n    start = time.time()\n\n    # Batches\n    for i, (imgs, caps, caplens) in enumerate(train_loader):\n        data_time.update(time.time() - start)\n\n        # Move to GPU, if available\n        imgs = imgs.to(device)\n        caps = caps.to(device)\n        caplens = caplens.to(device)\n\n        # Forward prop.\n        imgs = encoder(imgs)\n        scores, caps_sorted, decode_lengths, alphas, sort_ind = decoder(imgs, caps, caplens)\n\n        # Since we decoded starting with <start>, the targets are all words after <start>, up to <end>\n        targets = caps_sorted[:, 1:]\n\n        # Remove timesteps that we didn't decode at, or are pads\n        # pack_padded_sequence is an easy trick to do this\n        scores, _ = pack_padded_sequence(scores, decode_lengths, batch_first=True)\n        targets, _ = pack_padded_sequence(targets, decode_lengths, batch_first=True)\n\n        # Calculate loss\n        loss = criterion(scores, targets)\n\n        # Add doubly stochastic attention regularization\n        loss += alpha_c * ((1. - alphas.sum(dim=1)) ** 2).mean()\n\n        # Back prop.\n        decoder_optimizer.zero_grad()\n        if encoder_optimizer is not None:\n            encoder_optimizer.zero_grad()\n        loss.backward()\n\n        # Clip gradients\n        if grad_clip is not None:\n            clip_gradient(decoder_optimizer, grad_clip)\n            if encoder_optimizer is not None:\n                clip_gradient(encoder_optimizer, grad_clip)\n\n        # Update weights\n        decoder_optimizer.step()\n        if encoder_optimizer is not None:\n            encoder_optimizer.step()\n\n        # Keep track of metrics\n        top5 = accuracy(scores, targets, 5)\n        losses.update(loss.item(), sum(decode_lengths))\n        top5accs.update(top5, sum(decode_lengths))\n        batch_time.update(time.time() - start)\n\n        start = time.time()\n\n        # Print status\n        if i % print_freq == 0:\n            print('Epoch: [{0}][{1}/{2}]\\t'\n                  'Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n                  'Data Load Time {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n                  'Top-5 Accuracy {top5.val:.3f} ({top5.avg:.3f})'.format(epoch, i, len(train_loader),\n                                                                          batch_time=batch_time,\n                                                                          data_time=data_time, loss=losses,\n                                                                          top5=top5accs))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T14:06:04.471399Z","iopub.execute_input":"2024-12-30T14:06:04.471844Z","iopub.status.idle":"2024-12-30T14:06:04.839396Z","shell.execute_reply.started":"2024-12-30T14:06:04.471809Z","shell.execute_reply":"2024-12-30T14:06:04.837780Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"def validate(val_loader, encoder, decoder, criterion):\n    \"\"\"\n    Performs one epoch's validation.\n\n    :param val_loader: DataLoader for validation data.\n    :param encoder: encoder model\n    :param decoder: decoder model\n    :param criterion: loss layer\n    :return: BLEU-4 score\n    \"\"\"\n    decoder.eval()  # eval mode (no dropout or batchnorm)\n    if encoder is not None:\n        encoder.eval()\n\n    batch_time = AverageMeter()\n    losses = AverageMeter()\n    top5accs = AverageMeter()\n\n    start = time.time()\n\n    references = list()  # references (true captions) for calculating BLEU-4 score\n    hypotheses = list()  # hypotheses (predictions)\n\n    # explicitly disable gradient calculation to avoid CUDA memory error\n    # solves the issue #57\n    with torch.no_grad():\n        # Batches\n        for i, (imgs, caps, caplens, allcaps) in enumerate(val_loader):\n\n            # Move to device, if available\n            imgs = imgs.to(device)\n            caps = caps.to(device)\n            caplens = caplens.to(device)\n\n            # Forward prop.\n            if encoder is not None:\n                imgs = encoder(imgs)\n            scores, caps_sorted, decode_lengths, alphas, sort_ind = decoder(imgs, caps, caplens)\n\n            # Since we decoded starting with <start>, the targets are all words after <start>, up to <end>\n            targets = caps_sorted[:, 1:]\n\n            # Remove timesteps that we didn't decode at, or are pads\n            # pack_padded_sequence is an easy trick to do this\n            scores_copy = scores.clone()\n            scores, _ = pack_padded_sequence(scores, decode_lengths, batch_first=True)\n            targets, _ = pack_padded_sequence(targets, decode_lengths, batch_first=True)\n\n            # Calculate loss\n            loss = criterion(scores, targets)\n\n            # Add doubly stochastic attention regularization\n            loss += alpha_c * ((1. - alphas.sum(dim=1)) ** 2).mean()\n\n            # Keep track of metrics\n            losses.update(loss.item(), sum(decode_lengths))\n            top5 = accuracy(scores, targets, 5)\n            top5accs.update(top5, sum(decode_lengths))\n            batch_time.update(time.time() - start)\n\n            start = time.time()\n\n            if i % print_freq == 0:\n                print('Validation: [{0}/{1}]\\t'\n                      'Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n                      'Top-5 Accuracy {top5.val:.3f} ({top5.avg:.3f})\\t'.format(i, len(val_loader), batch_time=batch_time,\n                                                                                loss=losses, top5=top5accs))\n\n            # Store references (true captions), and hypothesis (prediction) for each image\n            # If for n images, we have n hypotheses, and references a, b, c... for each image, we need -\n            # references = [[ref1a, ref1b, ref1c], [ref2a, ref2b], ...], hypotheses = [hyp1, hyp2, ...]\n\n            # References\n            allcaps = allcaps[sort_ind]  # because images were sorted in the decoder\n            for j in range(allcaps.shape[0]):\n                img_caps = allcaps[j].tolist()\n                img_captions = list(\n                    map(lambda c: [w for w in c if w not in {word_map['<start>'], word_map['<pad>']}],\n                        img_caps))  # remove <start> and pads\n                references.append(img_captions)\n\n            # Hypotheses\n            _, preds = torch.max(scores_copy, dim=2)\n            preds = preds.tolist()\n            temp_preds = list()\n            for j, p in enumerate(preds):\n                temp_preds.append(preds[j][:decode_lengths[j]])  # remove pads\n            preds = temp_preds\n            hypotheses.extend(preds)\n\n            assert len(references) == len(hypotheses)\n\n        # Calculate BLEU-4 scores\n        bleu4 = corpus_bleu(references, hypotheses)\n\n        print(\n            '\\n * LOSS - {loss.avg:.3f}, TOP-5 ACCURACY - {top5.avg:.3f}, BLEU-4 - {bleu}\\n'.format(\n                loss=losses,\n                top5=top5accs,\n                bleu=bleu4))\n\n    return bleu4","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T14:06:08.463893Z","iopub.execute_input":"2024-12-30T14:06:08.464242Z","iopub.status.idle":"2024-12-30T14:06:08.477462Z","shell.execute_reply.started":"2024-12-30T14:06:08.464213Z","shell.execute_reply":"2024-12-30T14:06:08.476199Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T14:06:13.322784Z","iopub.execute_input":"2024-12-30T14:06:13.323174Z","iopub.status.idle":"2024-12-30T14:06:16.405668Z","shell.execute_reply.started":"2024-12-30T14:06:13.323142Z","shell.execute_reply":"2024-12-30T14:06:16.404156Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet101-63fe2227.pth\" to /root/.cache/torch/hub/checkpoints/resnet101-63fe2227.pth\n100%|██████████| 171M/171M [00:01<00:00, 160MB/s] \n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-263240bbee7e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-19-bd99dd7d9e7a>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m                                      std=[0.229, 0.224, 0.225])\n\u001b[1;32m     50\u001b[0m     train_loader = torch.utils.data.DataLoader(\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mCaptionDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'TRAIN'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCompose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         batch_size=batch_size, shuffle=True, num_workers=workers, pin_memory=True)\n\u001b[1;32m     53\u001b[0m     val_loader = torch.utils.data.DataLoader(\n","\u001b[0;32m<ipython-input-13-0ed884897530>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_folder, data_name, split, transform)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# Load encoded captions (completely into memory)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_CAPTIONS_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/working/TRAIN_CAPTIONS_coco_5_cap_per_img_5_min_word_freq.json'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/kaggle/working/TRAIN_CAPTIONS_coco_5_cap_per_img_5_min_word_freq.json'","output_type":"error"}],"execution_count":22},{"cell_type":"markdown","source":"## Predict","metadata":{}},{"cell_type":"code","source":"def caption_image_beam_search(encoder, decoder, image_path, word_map, beam_size=3):\n    \"\"\"\n    Reads an image and captions it with beam search.\n\n    :param encoder: encoder model\n    :param decoder: decoder model\n    :param image_path: path to image\n    :param word_map: word map\n    :param beam_size: number of sequences to consider at each decode-step\n    :return: caption, weights for visualization\n    \"\"\"\n\n    k = beam_size\n    vocab_size = len(word_map)\n\n    # Read image and process\n    img = imread(image_path)\n    if len(img.shape) == 2:\n        img = img[:, :, np.newaxis]\n        img = np.concatenate([img, img, img], axis=2)\n    img = imresize(img, (256, 256))\n    img = img.transpose(2, 0, 1)\n    img = img / 255.\n    img = torch.FloatTensor(img).to(device)\n    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                     std=[0.229, 0.224, 0.225])\n    transform = transforms.Compose([normalize])\n    image = transform(img)  # (3, 256, 256)\n\n    # Encode\n    image = image.unsqueeze(0)  # (1, 3, 256, 256)\n    encoder_out = encoder(image)  # (1, enc_image_size, enc_image_size, encoder_dim)\n    enc_image_size = encoder_out.size(1)\n    encoder_dim = encoder_out.size(3)\n\n    # Flatten encoding\n    encoder_out = encoder_out.view(1, -1, encoder_dim)  # (1, num_pixels, encoder_dim)\n    num_pixels = encoder_out.size(1)\n\n    # We'll treat the problem as having a batch size of k\n    encoder_out = encoder_out.expand(k, num_pixels, encoder_dim)  # (k, num_pixels, encoder_dim)\n\n    # Tensor to store top k previous words at each step; now they're just <start>\n    k_prev_words = torch.LongTensor([[word_map['<start>']]] * k).to(device)  # (k, 1)\n\n    # Tensor to store top k sequences; now they're just <start>\n    seqs = k_prev_words  # (k, 1)\n\n    # Tensor to store top k sequences' scores; now they're just 0\n    top_k_scores = torch.zeros(k, 1).to(device)  # (k, 1)\n\n    # Tensor to store top k sequences' alphas; now they're just 1s\n    seqs_alpha = torch.ones(k, 1, enc_image_size, enc_image_size).to(device)  # (k, 1, enc_image_size, enc_image_size)\n\n    # Lists to store completed sequences, their alphas and scores\n    complete_seqs = list()\n    complete_seqs_alpha = list()\n    complete_seqs_scores = list()\n\n    # Start decoding\n    step = 1\n    h, c = decoder.init_hidden_state(encoder_out)\n\n    # s is a number less than or equal to k, because sequences are removed from this process once they hit <end>\n    while True:\n\n        embeddings = decoder.embedding(k_prev_words).squeeze(1)  # (s, embed_dim)\n\n        awe, alpha = decoder.attention(encoder_out, h)  # (s, encoder_dim), (s, num_pixels)\n\n        alpha = alpha.view(-1, enc_image_size, enc_image_size)  # (s, enc_image_size, enc_image_size)\n\n        gate = decoder.sigmoid(decoder.f_beta(h))  # gating scalar, (s, encoder_dim)\n        awe = gate * awe\n\n        h, c = decoder.decode_step(torch.cat([embeddings, awe], dim=1), (h, c))  # (s, decoder_dim)\n\n        scores = decoder.fc(h)  # (s, vocab_size)\n        scores = F.log_softmax(scores, dim=1)\n\n        # Add\n        scores = top_k_scores.expand_as(scores) + scores  # (s, vocab_size)\n\n        # For the first step, all k points will have the same scores (since same k previous words, h, c)\n        if step == 1:\n            top_k_scores, top_k_words = scores[0].topk(k, 0, True, True)  # (s)\n        else:\n            # Unroll and find top scores, and their unrolled indices\n            top_k_scores, top_k_words = scores.view(-1).topk(k, 0, True, True)  # (s)\n\n        # Convert unrolled indices to actual indices of scores\n        prev_word_inds = top_k_words / vocab_size  # (s)\n        next_word_inds = top_k_words % vocab_size  # (s)\n\n        # Add new words to sequences, alphas\n        seqs = torch.cat([seqs[prev_word_inds], next_word_inds.unsqueeze(1)], dim=1)  # (s, step+1)\n        seqs_alpha = torch.cat([seqs_alpha[prev_word_inds], alpha[prev_word_inds].unsqueeze(1)],\n                               dim=1)  # (s, step+1, enc_image_size, enc_image_size)\n\n        # Which sequences are incomplete (didn't reach <end>)?\n        incomplete_inds = [ind for ind, next_word in enumerate(next_word_inds) if\n                           next_word != word_map['<end>']]\n        complete_inds = list(set(range(len(next_word_inds))) - set(incomplete_inds))\n\n        # Set aside complete sequences\n        if len(complete_inds) > 0:\n            complete_seqs.extend(seqs[complete_inds].tolist())\n            complete_seqs_alpha.extend(seqs_alpha[complete_inds].tolist())\n            complete_seqs_scores.extend(top_k_scores[complete_inds])\n        k -= len(complete_inds)  # reduce beam length accordingly\n\n        # Proceed with incomplete sequences\n        if k == 0:\n            break\n        seqs = seqs[incomplete_inds]\n        seqs_alpha = seqs_alpha[incomplete_inds]\n        h = h[prev_word_inds[incomplete_inds]]\n        c = c[prev_word_inds[incomplete_inds]]\n        encoder_out = encoder_out[prev_word_inds[incomplete_inds]]\n        top_k_scores = top_k_scores[incomplete_inds].unsqueeze(1)\n        k_prev_words = next_word_inds[incomplete_inds].unsqueeze(1)\n\n        # Break if things have been going on too long\n        if step > 50:\n            break\n        step += 1\n\n    i = complete_seqs_scores.index(max(complete_seqs_scores))\n    seq = complete_seqs[i]\n    alphas = complete_seqs_alpha[i]\n\n    return seq, alphas\n\n\ndef visualize_att(image_path, seq, alphas, rev_word_map, smooth=True):\n    \"\"\"\n    Visualizes caption with weights at every word.\n\n    Adapted from paper authors' repo: https://github.com/kelvinxu/arctic-captions/blob/master/alpha_visualization.ipynb\n\n    :param image_path: path to image that has been captioned\n    :param seq: caption\n    :param alphas: weights\n    :param rev_word_map: reverse word mapping, i.e. ix2word\n    :param smooth: smooth weights?\n    \"\"\"\n    image = Image.open(image_path)\n    image = image.resize([14 * 24, 14 * 24], Image.LANCZOS)\n\n    words = [rev_word_map[ind] for ind in seq]\n\n    for t in range(len(words)):\n        if t > 50:\n            break\n        plt.subplot(np.ceil(len(words) / 5.), 5, t + 1)\n\n        plt.text(0, 1, '%s' % (words[t]), color='black', backgroundcolor='white', fontsize=12)\n        plt.imshow(image)\n        current_alpha = alphas[t, :]\n        if smooth:\n            alpha = skimage.transform.pyramid_expand(current_alpha.numpy(), upscale=24, sigma=8)\n        else:\n            alpha = skimage.transform.resize(current_alpha.numpy(), [14 * 24, 14 * 24])\n        if t == 0:\n            plt.imshow(alpha, alpha=0)\n        else:\n            plt.imshow(alpha, alpha=0.8)\n        plt.set_cmap(cm.Greys_r)\n        plt.axis('off')\n    plt.show()\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='Show, Attend, and Tell - Tutorial - Generate Caption')\n\n    parser.add_argument('--img', '-i', help='path to image')\n    parser.add_argument('--model', '-m', help='path to model')\n    parser.add_argument('--word_map', '-wm', help='path to word map JSON')\n    parser.add_argument('--beam_size', '-b', default=5, type=int, help='beam size for beam search')\n    parser.add_argument('--dont_smooth', dest='smooth', action='store_false', help='do not smooth alpha overlay')\n\n    args = parser.parse_args()\n\n    # Load model\n    checkpoint = torch.load(args.model, map_location=str(device))\n    decoder = checkpoint['decoder']\n    decoder = decoder.to(device)\n    decoder.eval()\n    encoder = checkpoint['encoder']\n    encoder = encoder.to(device)\n    encoder.eval()\n\n    # Load word map (word2ix)\n    with open(args.word_map, 'r') as j:\n        word_map = json.load(j)\n    rev_word_map = {v: k for k, v in word_map.items()}  # ix2word\n\n    # Encode, decode with attention and beam search\n    seq, alphas = caption_image_beam_search(encoder, decoder, args.img, word_map, args.beam_size)\n    alphas = torch.FloatTensor(alphas)\n\n    # Visualize caption and attention of best sequence\n    visualize_att(args.img, seq, alphas, rev_word_map, args.smooth)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T14:00:13.452932Z","iopub.status.idle":"2024-12-30T14:00:13.453340Z","shell.execute_reply":"2024-12-30T14:00:13.453185Z"}},"outputs":[],"execution_count":null}]}