{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Dec  8 23:47:41 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 561.09                 Driver Version: 561.09         CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3060 ...  WDDM  |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   51C    P0             22W /   80W |       0MiB /   6144MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A     30780    C+G   ...nt.CBS_cw5n1h2txyewy\\SearchHost.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchmetrics\n",
      "  Downloading torchmetrics-1.6.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy>1.20.0 in c:\\programdata\\anaconda3\\envs\\handwriting_rec\\lib\\site-packages (from torchmetrics) (2.1.1)\n",
      "Requirement already satisfied: packaging>17.1 in c:\\programdata\\anaconda3\\envs\\handwriting_rec\\lib\\site-packages (from torchmetrics) (24.1)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\programdata\\anaconda3\\envs\\handwriting_rec\\lib\\site-packages (from torchmetrics) (2.4.1+cu118)\n",
      "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
      "  Downloading lightning_utilities-0.11.9-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\envs\\handwriting_rec\\lib\\site-packages (from lightning-utilities>=0.8.0->torchmetrics) (74.1.2)\n",
      "Requirement already satisfied: typing-extensions in c:\\programdata\\anaconda3\\envs\\handwriting_rec\\lib\\site-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.12.2)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\envs\\handwriting_rec\\lib\\site-packages (from torch>=2.0.0->torchmetrics) (3.16.1)\n",
      "Requirement already satisfied: sympy in c:\\programdata\\anaconda3\\envs\\handwriting_rec\\lib\\site-packages (from torch>=2.0.0->torchmetrics) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\envs\\handwriting_rec\\lib\\site-packages (from torch>=2.0.0->torchmetrics) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\envs\\handwriting_rec\\lib\\site-packages (from torch>=2.0.0->torchmetrics) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\programdata\\anaconda3\\envs\\handwriting_rec\\lib\\site-packages (from torch>=2.0.0->torchmetrics) (2024.9.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\envs\\handwriting_rec\\lib\\site-packages (from jinja2->torch>=2.0.0->torchmetrics) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\programdata\\anaconda3\\envs\\handwriting_rec\\lib\\site-packages (from sympy->torch>=2.0.0->torchmetrics) (1.3.0)\n",
      "Downloading torchmetrics-1.6.0-py3-none-any.whl (926 kB)\n",
      "   ---------------------------------------- 0.0/926.4 kB ? eta -:--:--\n",
      "   ---------------------- ----------------- 524.3/926.4 kB 4.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 926.4/926.4 kB 4.7 MB/s eta 0:00:00\n",
      "Downloading lightning_utilities-0.11.9-py3-none-any.whl (28 kB)\n",
      "Installing collected packages: lightning-utilities, torchmetrics\n",
      "Successfully installed lightning-utilities-0.11.9 torchmetrics-1.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torchmetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image captioning Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import math\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "\n",
    "from torchmetrics.text import BLEUScore\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "import time\n",
    "\n",
    "import wandb\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 128\n",
    "\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        DEVICE = x.DEVICE\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, DEVICE=DEVICE) * -emb)\n",
    "        emb = x[:, None] * emb[None, :]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        return emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_patches(image_tensor, patch_size=16):\n",
    "    # Get the dimensions of the image tensor\n",
    "    b, c, h, w = image_tensor.size()\n",
    "\n",
    "    # Define the Unfold layer with appropriate parameters\n",
    "    unfold = torch.nn.Unfold(kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    # Apply Unfold to the image tensor\n",
    "    unfolded = unfold(image_tensor)\n",
    "\n",
    "    # Reshape the unfolded tensor to match the desired output shape\n",
    "    # Output shape: BxLxH, where L is the number of patches in each dimension\n",
    "    unfolded = unfolded.transpose(1, 2).reshape(b, -1, c * patch_size * patch_size)\n",
    "\n",
    "    return unfolded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module): #base on VIT\n",
    "    def __init__(self, image_size, channels_in, patch_size=16, hidden_size=128,\n",
    "                 num_layers=3, num_heads=4):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "        self.fc_in = nn.Linear(channels_in * patch_size * patch_size, hidden_size)\n",
    "\n",
    "        seq_length = (image_size // patch_size) ** 2\n",
    "        self.pos_embedding = nn.Parameter(torch.empty(1, seq_length,\n",
    "                                                      hidden_size).normal_(std=0.02))\n",
    "\n",
    "        # Create multiple transformer blocks as layers\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_size, nhead=num_heads, dim_feedforward= hidden_size*4, \n",
    "                                                   batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer=encoder_layer, num_layers=num_layers)\n",
    "\n",
    "\n",
    "    def forward(self, image):\n",
    "        b = image.shape[0]\n",
    "\n",
    "        patch_seq = extract_patches(image, patch_size=self.patch_size)\n",
    "        patch_emb = self.fc_in(patch_seq)\n",
    "\n",
    "        # Add a unique embedding to each token embedding\n",
    "        embs = patch_emb + self.pos_embedding\n",
    "\n",
    "        # Pass the embeddings through each transformer block\n",
    "        output = self.transformer_encoder(embs)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTModel\n",
    "\n",
    "class ImageEmbedding(nn.Module):\n",
    "    def __init__(self, vit_model=\"google/vit-base-patch16-224-in21k\"):\n",
    "        super(ImageEmbedding, self).__init__()\n",
    "        self.vit = ViTModel.from_pretrained(vit_model)\n",
    "        \n",
    "        # Use only the patch embedding layer\n",
    "        self.patch_embed = self.vit.embeddings.patch_embeddings\n",
    "        self.pos_embed = self.vit.embeddings.position_embeddings\n",
    "\n",
    "    def forward(self, images):\n",
    "        # Extract patch embeddings\n",
    "        patch_embeds = self.patch_embed(images)\n",
    "        \n",
    "        # Add positional embeddings\n",
    "        embs = patch_embeds + self.pos_embed\n",
    "        return embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder2(nn.Module): #base on VIT\n",
    "    def __init__(self, hidden_size=128, num_layers=3, num_heads=4):\n",
    "        super(Encoder2, self).__init__()\n",
    "\n",
    "        # Create multiple transformer blocks as layers\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_size, nhead=num_heads, dim_feedforward= hidden_size*4, \n",
    "                                                   batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer=encoder_layer, num_layers=num_layers)\n",
    "\n",
    "\n",
    "    def forward(self, image):\n",
    "        b = image.shape[0]\n",
    "\n",
    "        embs = ImageEmbedding()\n",
    "\n",
    "        # Pass the embeddings through each transformer block\n",
    "        output = self.transformer_encoder(embs)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module): #base on BERT\n",
    "    def __init__(self, num_emb, hidden_size=128, num_layers=3, num_heads=4):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        # Create an embedding layer for tokens\n",
    "        self.embedding = nn.Embedding(num_emb, hidden_size)\n",
    "        # Initialize the embedding weights\n",
    "        self.embedding.weight.data = 0.001 * self.embedding.weight.data\n",
    "\n",
    "        # Initialize sinusoidal positional embeddings\n",
    "        self.pos_emb = SinusoidalPosEmb(hidden_size)\n",
    "\n",
    "        # Create multiple transformer blocks as layers\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=hidden_size, nhead=num_heads,dim_feedforward= hidden_size*4, batch_first=True)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Define a linear layer for output prediction\n",
    "        self.fc_out = nn.Linear(hidden_size, num_emb)\n",
    "\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "    def forward(self, input_seq, encoder_output, input_padding_mask=None,\n",
    "                encoder_padding_mask=None):\n",
    "        # Embed the input sequence\n",
    "        input_embs = self.embedding(input_seq)\n",
    "        b, l, h = input_embs.shape\n",
    "\n",
    "        # Add positional embeddings to the input embeddings\n",
    "        seq_indx = torch.arange(l, DEVICE=input_seq.DEVICE)\n",
    "        pos_emb = self.pos_emb(seq_indx).reshape(1, l, h).expand(b, l, h)\n",
    "        embs = input_embs + pos_emb\n",
    "\n",
    "        # Pass the embeddings through each transformer block\n",
    "        output = self.transformer_decoder(tgt = embs, memory=encoder_output, memory_mask=None, \n",
    "                                          tgt_key_padding_mask=input_padding_mask, memory_key_padding_mask=encoder_padding_mask,\n",
    "                                          tgt_is_causal=True, memory_is_causal=False)\n",
    "\n",
    "        output = self.softmax(self.fc_out(output))\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "class TextEmbedding(nn.Module):\n",
    "    def __init__(self, bert_model=\"bert-base-uncased\"):\n",
    "        super(TextEmbedding, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_model)\n",
    "        \n",
    "        # Extract word and positional embeddings\n",
    "        self.embedding = self.bert.embeddings.word_embeddings\n",
    "\n",
    "    def forward(self, tokens):\n",
    "\n",
    "        tokens = tokens.to(self.embedding.weight.device)\n",
    "        \n",
    "        text_embeds = self.embedding(tokens)\n",
    "        return text_embeds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder2(nn.Module): #base on BERT\n",
    "    def __init__(self, num_emb, hidden_size=128, num_layers=3, num_heads=4):\n",
    "        super(Decoder2, self).__init__()\n",
    "\n",
    "        self.embedding = TextEmbedding\n",
    "\n",
    "        # Initialize sinusoidal positional embeddings\n",
    "        self.pos_emb = SinusoidalPosEmb(hidden_size)\n",
    "\n",
    "        # Create multiple transformer blocks as layers\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=hidden_size, nhead=num_heads,dim_feedforward= hidden_size*4, batch_first=True)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Define a linear layer for output prediction\n",
    "        self.fc_out = nn.Linear(hidden_size, num_emb)\n",
    "\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "    def forward(self, input_seq, encoder_output, input_padding_mask=None,\n",
    "                encoder_padding_mask=None):\n",
    "        # Embed the input sequence\n",
    "        input_embs = self.embedding(input_seq)\n",
    "        b, l, h = input_embs.shape\n",
    "\n",
    "        # Add positional embeddings to the input embeddings\n",
    "        seq_indx = torch.arange(l, DEVICE=input_seq.DEVICE)\n",
    "        pos_emb = self.pos_emb(seq_indx).reshape(1, l, h).expand(b, l, h)\n",
    "        embs = input_embs + pos_emb\n",
    "\n",
    "        # Pass the embeddings through each transformer block\n",
    "        output = self.transformer_decoder(tgt = embs, memory=encoder_output, memory_mask=None, \n",
    "                                          tgt_key_padding_mask=input_padding_mask, memory_key_padding_mask=encoder_padding_mask,\n",
    "                                          tgt_is_causal=True, memory_is_causal=False)\n",
    "\n",
    "        output = self.softmax(self.fc_out(output))\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode-Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, image_size, channels_in, num_emb, patch_size=16,\n",
    "                 hidden_size=128, num_layers=(3, 3), num_heads=4):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "\n",
    "        # Create an encoder and decoder with specified parameters\n",
    "        self.encoder = Encoder(image_size=image_size, channels_in=channels_in,\n",
    "                                     patch_size=patch_size, hidden_size=hidden_size,\n",
    "                                     num_layers=num_layers[0], num_heads=num_heads)\n",
    "\n",
    "        self.decoder = Decoder(num_emb=num_emb, hidden_size=hidden_size,\n",
    "                               num_layers=num_layers[1], num_heads=num_heads)\n",
    "\n",
    "    def forward(self, input_image, target_seq, padding_mask):\n",
    "        # Generate padding masks for the target sequence\n",
    "        bool_padding_mask = padding_mask == 0\n",
    "\n",
    "        # Encode the input sequence\n",
    "        encoded_seq = self.encoder(image=input_image)\n",
    "\n",
    "        # Decode the target sequence using the encoded sequence\n",
    "        decoded_seq = self.decoder(input_seq=target_seq,\n",
    "                                   encoder_output=encoded_seq,\n",
    "                                   input_padding_mask=bool_padding_mask)\n",
    "        return decoded_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder2(nn.Module):\n",
    "    def __init__(self, num_emb,\n",
    "                 hidden_size=128, num_layers=(3, 3), num_heads=4):\n",
    "        super(EncoderDecoder2, self).__init__()\n",
    "\n",
    "        # Create an encoder and decoder with specified parameters\n",
    "        self.encoder = Encoder2(hidden_size=hidden_size,\n",
    "                                     num_layers=num_layers[0], num_heads=num_heads)\n",
    "\n",
    "        self.decoder = Decoder2(num_emb=num_emb, hidden_size=hidden_size,\n",
    "                               num_layers=num_layers[1], num_heads=num_heads)\n",
    "\n",
    "    def forward(self, input_image, target_seq, padding_mask):\n",
    "        # Generate padding masks for the target sequence\n",
    "        bool_padding_mask = padding_mask == 0\n",
    "\n",
    "        # Encode the input sequence\n",
    "        encoded_seq = self.encoder(image=input_image)\n",
    "\n",
    "        # Decode the target sequence using the encoded sequence\n",
    "        decoded_seq = self.decoder(input_seq=target_seq,\n",
    "                                   encoder_output=encoded_seq,\n",
    "                                   input_padding_mask=bool_padding_mask)\n",
    "        return decoded_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrain Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTModel, ViTConfig\n",
    "\n",
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, pretrained_model=\"google/vit-base-patch16-224-in21k\"):\n",
    "        super(ImageEncoder, self).__init__()\n",
    "        self.vit = ViTModel.from_pretrained(pretrained_model)\n",
    "        \n",
    "    def forward(self, images):\n",
    "        vit_output = self.vit(pixel_values=images)\n",
    "        return vit_output.last_hidden_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "class CaptionDecoder(nn.Module):\n",
    "    def __init__(self, pretrained_model=\"bert-base-uncased\"):\n",
    "        super(CaptionDecoder, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(pretrained_model)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(pretrained_model)\n",
    "\n",
    "    def forward(self, captions, encoder_outputs):\n",
    "        # Tokenize captions\n",
    "        tokens = self.tokenizer(captions, padding=True, return_tensors=\"pt\").input_ids\n",
    "        tokens = tokens.to(encoder_outputs.device)\n",
    "\n",
    "        # Pass through BERT, using image features as encoder outputs\n",
    "        outputs = self.bert(\n",
    "            input_ids=tokens,\n",
    "            encoder_hidden_states=encoder_outputs,\n",
    "            encoder_attention_mask=torch.ones_like(tokens)\n",
    "        )\n",
    "        return outputs.last_hidden_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder-Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptioningModel(nn.Module):\n",
    "    def __init__(self, vit_model=\"google/vit-base-patch16-224-in21k\",\n",
    "                 bert_model=\"bert-base-uncased\"):\n",
    "        super(ImageCaptioningModel, self).__init__()\n",
    "        self.encoder = ImageEncoder(pretrained_model=vit_model)\n",
    "        self.decoder = CaptionDecoder(pretrained_model=bert_model)\n",
    "\n",
    "    def forward(self, images, captions):\n",
    "        encoder_outputs = self.encoder(images)\n",
    "        decoder_outputs = self.decoder(captions, encoder_outputs)\n",
    "        return decoder_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(1 if torch.cuda.is_available() else 'cpu')\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "\n",
    "epochs = 200\n",
    "\n",
    "model_path = \"/model/Transformer_model.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 192\n",
    "\n",
    "# Number of Transformer blocks for the (Encoder, Decoder)\n",
    "num_layers = (6, 6)\n",
    "\n",
    "# MultiheadAttention Heads\n",
    "num_heads = 8\n",
    "\n",
    "# Size of the patches\n",
    "patch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EncoderDecoder(image_size=image_size, channels_in=test_images.shape[1],\n",
    "                                     num_emb=tokenizer.vocab_size, patch_size=patch_size,\n",
    "                                     num_layers=num_layers,hidden_size=hidden_size,\n",
    "                                     num_heads=num_heads).to(DEVICE)\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "#summary(model, (3, 256, 256))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wandb para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = \"Image_Captioning\"\n",
    "RESUME = \"allow\"\n",
    "WANDB_KEY = \"d9d14819dddd8a35a353b5c0b087e0f60d717140\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenDrop(nn.Module):\n",
    "    \"\"\"For a batch of tokens indices, randomly replace a non-specical token.\n",
    "\n",
    "    Args:\n",
    "        prob (float): probability of dropping a token\n",
    "        blank_token (int): index for the blank token\n",
    "        num_special (int): Number of special tokens, assumed to be at the start of the vocab\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, prob=0.1, blank_token=1, eos_token=102):\n",
    "        self.prob = prob\n",
    "        self.eos_token = eos_token\n",
    "        self.blank_token = blank_token\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        # Randomly sample a bernoulli distribution with p=prob\n",
    "        # to create a mask where 1 means we will replace that token\n",
    "        mask = torch.bernoulli(self.prob * torch.ones_like(sample)).long()\n",
    "\n",
    "        # only replace if the token is not the eos token\n",
    "        can_drop = (~(sample == self.eos_token)).long()\n",
    "        mask = mask * can_drop\n",
    "\n",
    "        # Do not replace the sos tokens\n",
    "        mask[:, 0] = torch.zeros_like(mask[:, 0]).long()\n",
    "\n",
    "        replace_with = (self.blank_token * torch.ones_like(sample)).long()\n",
    "\n",
    "        sample_out = (1 - mask) * sample + mask * replace_with\n",
    "\n",
    "        return sample_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "td = TokenDrop(0.5)\n",
    "\n",
    "# Initialize the training loss logger\n",
    "training_loss_logger = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login(\n",
    "    key = \"d9d14819dddd8a35a353b5c0b087e0f60d717140\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "    project=PROJECT,\n",
    "    resume=RESUME,\n",
    "    name=\"init_transformer\",\n",
    "    config={\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"epochs\": epochs,\n",
    "        \"batch_size\": batch_size,\n",
    "    },\n",
    ")\n",
    "wandb.watch(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=15, factor=0.1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = float('inf')\n",
    "bleu_metric = BLEUScore(n_gram=4, smooth=True).to(DEVICE)\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, loss_fn, DEVICE):\n",
    "\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    # Iterate over the training data loader\n",
    "    for images, captions in tqdm(dataloader, desc=\"Training\", leave=False):\n",
    "\n",
    "        images = images.to(DEVICE)\n",
    "\n",
    "        # Tokenize and pre-process the captions\n",
    "        tokens = tokenizer(captions, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        token_ids = tokens['input_ids'].to(DEVICE)\n",
    "        padding_mask = tokens['attention_mask'].to(DEVICE)\n",
    "        b = token_ids.shape[0]\n",
    "\n",
    "        # Shift right the input sequence to create the target sequence\n",
    "        target_ids = torch.cat((token_ids[:, 1:],\n",
    "                                torch.zeros(b, 1, DEVICE=DEVICE).long()), 1)\n",
    "\n",
    "        tokens_in = td(token_ids)\n",
    "        with torch.cuda.amp.autocast():\n",
    "            # Forward pass\n",
    "            pred = model(images, tokens_in, padding_mask=padding_mask)\n",
    "            loss = (loss_fn(pred.transpose(1, 2), target_ids) * padding_mask).mean()\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "\n",
    "    return epoch_loss\n",
    "\n",
    "def validate_epoch(model, dataloader, loss_fn, DEVICE):\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    bleu_metric.reset()  # Reset BLEU metric before validation\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, captions in tqdm(dataloader, desc=\"Validation\", leave=False):\n",
    "            images = images.to(DEVICE)\n",
    "            tokens = tokenizer(captions, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "            token_ids = tokens['input_ids'].to(DEVICE)\n",
    "            padding_mask = tokens['attention_mask'].to(DEVICE)\n",
    "            b = token_ids.shape[0]\n",
    "\n",
    "            target_ids = torch.cat((token_ids[:, 1:], torch.zeros(b, 1, DEVICE=DEVICE).long()), 1)\n",
    "            tokens_in = td(token_ids)\n",
    "\n",
    "            # Forward pass\n",
    "            with torch.cuda.amp.autocast():\n",
    "                pred = model(images, tokens_in, padding_mask=padding_mask)\n",
    "\n",
    "            # Compute validation loss\n",
    "            val_loss = (loss_fn(pred.transpose(1, 2), target_ids) * padding_mask).mean()\n",
    "            total_val_loss += val_loss.item()\n",
    "\n",
    "            # Decode predictions and targets\n",
    "            pred_ids = torch.argmax(pred, dim=2)\n",
    "            pred_texts = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "            target_texts = tokenizer.batch_decode(target_ids, skip_special_tokens=True)\n",
    "\n",
    "            # Update BLEU score\n",
    "            bleu_metric.update(pred_texts, target_texts)\n",
    "\n",
    "    epoch_loss = total_val_loss / len(dataloader)\n",
    "    avg_bleu_score = bleu_metric.compute()\n",
    "\n",
    "    return [epoch_loss, avg_bleu_score]\n",
    "\n",
    "\n",
    "for epoch in trange(0, epochs, leave=False, desc=\"Epoch\"):\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, loss_fn, DEVICE)\n",
    "    val_loss, val_bleu = validate_epoch(model, val_loader, loss_fn, DEVICE)\n",
    "\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    epoch_time = time.time() - start_time\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Time: {epoch_time:.2f}s, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, Dice_Coefficient: {val_bleu:.4f}, Learning Rate: {current_lr:.8f}\")\n",
    "\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print(f\"New best checkpoint saved with val_loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Log results to WandB\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch,\n",
    "        \"train_loss\": train_loss,\n",
    "        \"val_loss\": val_loss,\n",
    "        \"Dice_Coefficient\": val_bleu,\n",
    "        \"learing_rate\": current_lr,\n",
    "    })\n",
    "\n",
    "wandb.finish()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "handwriting_rec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
